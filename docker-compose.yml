version: "3.8"


services:
  airflow:
    container_name: airflow
    image: airflow
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    env_file:
      - ./docker/mlflow.env
      - ./docker/airflow.env
    ports:
      - 8080:8080
    volumes:
      - ./src/dags:/opt/airflow/dags
    depends_on:
      - db
      - mlflow

  mlflow:
    container_name: mlflow
    image: mlflow
    build:
      context: ./docker/mlflow
      dockerfile: Dockerfile
    env_file: ./docker/mlflow.env
    ports:
      - 8000:8000
    volumes:
      - ./src:/home/mlflow/mlflow
    depends_on:
      - db

  db:
    container_name: db
    image: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - 5433:5432

  # hadoop services

  hadoop-namenode:
    container_name: hadoop-namenode
    image: hadoop-namenode
    build:
      context: ./docker/hadoop/hadoop-namenode
      dockerfile: Dockerfile
    restart: always
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    ports:
      - "32763:9870"
    volumes:
      - ./mnt/hadoop/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=hadoop_cluster
    healthcheck:
      test: [ "CMD", "nc", "-z", "namenode", "9870" ]
      timeout: 45s
      interval: 10s
      retries: 10
